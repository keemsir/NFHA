{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN(Generative Adversarial Networks) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, LeakyReLU, BatchNormalization, concatenate\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold, train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.genfromtxt(\"./oneD_power.csv\", delimiter=\",\", skip_header=1)\n",
    "\n",
    "# X = data[:, :-1]\n",
    "X = data[:, -1]\n",
    "y = data[:, :-1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "# Scaler2 = StandardScaler()\n",
    "\n",
    "# X_train_scaled = scaler.fit_transform(X_train)\n",
    "# X_test_scaled = scaler.fit_transform(X_test)\n",
    "X_scaled = X_train\n",
    "y_scaled = y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200000, 6)\n",
      "(50000, 6)\n",
      "(200000,)\n",
      "(50000,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter setting\n",
    "latent_dim = 100\n",
    "output_dim = y.shape[1]\n",
    "\n",
    "# Generator model\n",
    "def build_generator():\n",
    "    model = Sequential([\n",
    "        Dense(128, input_dim=latent_dim+1),\n",
    "        LeakyReLU(alpha=0.2),\n",
    "        BatchNormalization(momentum=0.8),\n",
    "        Dense(256),\n",
    "        LeakyReLU(alpha=0.2),\n",
    "        BatchNormalization(momentum=0.8),\n",
    "        Dense(512),\n",
    "        LeakyReLU(alpha=0.2),\n",
    "        BatchNormalization(momentum=0.8),\n",
    "        Dense(1, activation='linear')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Discriminator model\n",
    "def build_discriminator():\n",
    "    model = Sequential([\n",
    "        Dense(512, input_dim=1+1),\n",
    "        LeakyReLU(alpha=0.2),\n",
    "        Dense(256),\n",
    "        LeakyReLU(alpha=0.2),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# init\n",
    "optimizer = Adam(0.0002, 0.5)\n",
    "generator = build_generator()\n",
    "discriminator = build_discriminator()\n",
    "\n",
    "# Discriminator comfile\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# Combined model (Generator + Discriminator)\n",
    "z = Input(shape=(latent_dim,))\n",
    "power_input = Input(shape=(1,))\n",
    "gen_input = concatenate([z, power_input])\n",
    "generated_param = generator(gen_input)\n",
    "\n",
    "discriminator.trainable = False\n",
    "validity = discriminator(concatenate([generated_param, power_input]))\n",
    "\n",
    "combined = Model([z, power_input], validity)\n",
    "combined.compile(loss='binary_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/home/keem/anaconda3/lib/python3.10/site-packages/keras/engine/training.py\", line 2137, in predict_function  *\n        return step_function(self, iterator)\n    File \"/home/keem/anaconda3/lib/python3.10/site-packages/keras/engine/training.py\", line 2123, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/keem/anaconda3/lib/python3.10/site-packages/keras/engine/training.py\", line 2111, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/home/keem/anaconda3/lib/python3.10/site-packages/keras/engine/training.py\", line 2079, in predict_step\n        return self(x, training=False)\n    File \"/home/keem/anaconda3/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/home/keem/anaconda3/lib/python3.10/site-packages/keras/engine/input_spec.py\", line 216, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Layer \"sequential_4\" expects 1 input(s), but it received 2 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(32, 100) dtype=float32>, <tf.Tensor 'IteratorGetNext:1' shape=(32,) dtype=float32>]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m real_params \u001b[39m=\u001b[39m y_scaled[idx]\n\u001b[1;32m     15\u001b[0m noise \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mnormal(\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, (batch_size, latent_dim))\n\u001b[0;32m---> 16\u001b[0m gen_params \u001b[39m=\u001b[39m generator\u001b[39m.\u001b[39;49mpredict([noise, power_batch])\n\u001b[1;32m     18\u001b[0m d_loss_real \u001b[39m=\u001b[39m discriminator\u001b[39m.\u001b[39mtrain_on_batch(np\u001b[39m.\u001b[39mconcatenate([real_params, power_batch], axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m), valid)\n\u001b[1;32m     19\u001b[0m d_loss_fake \u001b[39m=\u001b[39m discriminator\u001b[39m.\u001b[39mtrain_on_batch(np\u001b[39m.\u001b[39mconcatenate([gen_params, power_batch], axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m), fake)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_fileq8bwj9hv.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__predict_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/home/keem/anaconda3/lib/python3.10/site-packages/keras/engine/training.py\", line 2137, in predict_function  *\n        return step_function(self, iterator)\n    File \"/home/keem/anaconda3/lib/python3.10/site-packages/keras/engine/training.py\", line 2123, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/keem/anaconda3/lib/python3.10/site-packages/keras/engine/training.py\", line 2111, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/home/keem/anaconda3/lib/python3.10/site-packages/keras/engine/training.py\", line 2079, in predict_step\n        return self(x, training=False)\n    File \"/home/keem/anaconda3/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/home/keem/anaconda3/lib/python3.10/site-packages/keras/engine/input_spec.py\", line 216, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Layer \"sequential_4\" expects 1 input(s), but it received 2 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(32, 100) dtype=float32>, <tf.Tensor 'IteratorGetNext:1' shape=(32,) dtype=float32>]\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter setting\n",
    "epochs = 10 # 10000\n",
    "batch_size = 64\n",
    "sample_interval = 200\n",
    "\n",
    "# training\n",
    "valid = np.ones((batch_size, 1))\n",
    "fake = np.zeros((batch_size, 1))\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    idx = np.random.randint(0, X_scaled.shape[0], batch_size)\n",
    "    power_batch = X_scaled[idx]\n",
    "    real_params = y_scaled[idx]\n",
    "\n",
    "    noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "    gen_params = generator.predict([noise, power_batch])\n",
    "\n",
    "    d_loss_real = discriminator.train_on_batch(np.concatenate([real_params, power_batch], axis=1), valid)\n",
    "    d_loss_fake = discriminator.train_on_batch(np.concatenate([gen_params, power_batch], axis=1), fake)\n",
    "    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "    g_loss = combined.train_on_batch([noise, power_batch], valid)\n",
    "\n",
    "    if epoch % sample_interval == 0:\n",
    "        print(f\"{epoch} [D loss: {d_loss[0]}] [D accuracy: {100*d_loss[1]}] [G loss: {g_loss}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
